from itertools import combinations
from typing import List, Union, Dict, Tuple, Any, List

import numpy as np
from numpy import ndarray
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc
from sklearn.preprocessing import label_binarize


class ClassificationMetricsForSingle:
    """Calculate different figures figures for classification,
    which are only legal for the methods of one class for one data."""

    def __init__(self, pps: np.ndarray, gts: np.ndarray, classes: List[str]):
        """
        Args:
            pps: all items' prediction probabilities with the shape of [N, C], and sum(pps[i, :]) = 1
            gts: all items' true labels with the shape of [N,], where the value should be between [0, C)
            classes: list with categories' name, and the index of a specific category in the list is corresponding
            to the prediction index.
        """

        self.pps = pps
        self.gts = gts
        self.num_class = pps.shape[1]
        self.classes = classes
        # tps is true probabilities, which are generated by encoding gts in one-hot manner, and its shape is [N, C]
        self.tps = label_binarize(gts, classes=[i for i in range(self.num_class)])
        self.pair_list = list(combinations([i for i in range(self.num_class)], 2))

        # Ensure the calculation process for a specified figures is only called once
        self._cm = None
        self._cache_cm = False
        self._as = None
        self._cache_as = None
        self._fs = None
        self._cache_fs = False
        self._pr = None
        self._cache_pr = False
        self._roc = None
        self._cache_roc = False

    def _confusion_matrix(self, pps, tps) -> np.ndarray:
        num_class = self.num_class
        confusion_matrix = np.zeros((num_class + 1, num_class + 1), dtype=np.float64)

        for idx in range(pps.shape[0]):
            predict_label = int(np.argmax(pps[idx, :]))
            gt_label = int(np.argmax(tps[idx, :]))
            confusion_matrix[gt_label, predict_label] += 1

        # Calculate precisions of different classes, where the precisions are not normalized
        confusion_matrix[-1, :-1] = np.sum(confusion_matrix[:-1, :-1], axis=0)

        # Calculate recalls of different classes, where the recalls are not normalized
        confusion_matrix[:-1, -1] = np.sum(confusion_matrix[:-1, :-1], axis=1)

        # Calculate number of data
        confusion_matrix[-1, -1] = pps.shape[0]

        return confusion_matrix

    def _accuracy(self, confusion_matrix) -> float:
        cor = np.sum(np.diag(confusion_matrix[:-1, :-1]))
        accuracy = 1.0 * cor / confusion_matrix[-1, -1]

        return accuracy

    def _precision_recall(self, pps, tps) -> Dict[str, Dict[Union[Any, str], np.ndarray]]:
        precision = dict()
        recall = dict()
        average_precision = dict()
        for i in range(self.num_class):
            precision[i], recall[i], _ = precision_recall_curve(tps[:, i], pps[:, i])
            average_precision[i] = average_precision_score(tps[:, i], pps[:, i])
        # A "micro-average": quantifying score on all classes jointly
        precision['micro'], recall["micro"], _ = precision_recall_curve(tps.ravel(), pps.ravel())
        average_precision["micro"] = average_precision_score(pps, tps, average="micro")

        pr = dict(precision=precision, recall=recall, average_precision=average_precision)

        return pr

    def _f1_score(self, confusion_matrix) -> Dict[str, float]:
        recall = confusion_matrix[:-1, -1]
        precision = confusion_matrix[-1, :-1]
        f1_score = 2 * precision * recall / (precision + recall)
        f1_score = {self.classes[index]: f1 for index, f1 in enumerate(f1_score)}
        f1_score['F1'] = 1.0 * sum(f1_score.values()) / len(f1_score)

        return f1_score

    def _roc_ovr(self, pps, tps) -> Dict[str, Dict[str, np.ndarray]]:
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(self.num_class):
            fpr[i], tpr[i], _ = roc_curve(tps[:, i], pps[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # ROC curve using micro-averaged OvR
        fpr["micro"], tpr["micro"], _ = roc_curve(tps.ravel(), pps.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        # ROC curve using the OvR macro-average
        fpr_grid = np.linspace(0.0, 1.0, 1000)

        # Interpolate all ROC curves at these points
        mean_tpr = np.zeros_like(fpr_grid)

        for i in range(self.num_class):
            mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation

        # Average it and compute AUC
        mean_tpr /= self.num_class

        fpr["macro"] = fpr_grid
        tpr["macro"] = mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        roc = dict(fpr=fpr, tpr=tpr, auc=roc_auc)

        return roc

    def _roc_ovo(self, pps, gts) -> Dict[str, Dict[Tuple[int, int], Dict[Any, Any]]]:

        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        fpr_grid = np.linspace(0.0, 1.0, 1000)
        for ix, (label_a, label_b) in enumerate(self.pair_list):
            fpr[(label_a, label_b)] = dict()
            tpr[(label_a, label_b)] = dict()
            roc_auc[(label_a, label_b)] = dict()

            a_mask = gts == label_a
            b_mask = gts == label_b
            ab_mask = np.logical_or(a_mask, b_mask)

            a_true = a_mask[ab_mask]
            b_true = b_mask[ab_mask]

            fpr_a, tpr_a, _ = roc_curve(a_true, pps[ab_mask, label_a])
            fpr_b, tpr_b, _ = roc_curve(b_true, pps[ab_mask, label_b])

            auc_a = auc(fpr_a, tpr_a)
            auc_b = auc(fpr_b, tpr_b)

            fpr[(label_a, label_b)][label_a] = fpr_a
            fpr[(label_a, label_b)][label_b] = fpr_b
            tpr[(label_a, label_b)][label_a] = tpr_a
            tpr[(label_a, label_b)][label_b] = tpr_b
            roc_auc[(label_a, label_b)][label_a] = auc_a
            roc_auc[(label_a, label_b)][label_b] = auc_b

            mean_tpr = (np.interp(fpr_grid, fpr_a, tpr_a) + np.interp(fpr_grid, fpr_b, tpr_b)) / 2
            mean_auc = auc(fpr_grid, mean_tpr)

            fpr[(label_a, label_b)]['macro'] = fpr_grid
            tpr[(label_a, label_b)]['macro'] = mean_tpr
            roc_auc[(label_a, label_b)]['macro'] = mean_auc

        roc = dict(fpr=fpr, tpr=tpr, auc=roc_auc)

        return roc

    @property
    def confusion_matrix(self) -> np.ndarray:
        if not self._cache_cm:
            confusion_matrix = self._confusion_matrix(self.pps, self.gts)
            self._cm = confusion_matrix
            self._cache_cm = True

        return self._cm

    @property
    def accuracy(self) -> float:
        if not self._cache_as:
            accuracy = self._accuracy(self.confusion_matrix)
            self._as = accuracy
            self._cache_as = True
        return self._as

    @property
    def f1_score(self) -> Dict[str, float]:
        if not self._cache_fs:
            f1_score = self._f1_score(self.confusion_matrix)
            self._fs = f1_score
            self._cache_fs = True

        return self._fs

    @property
    def precision_recall(self) -> Dict[str, Dict[Union[Any, str], np.ndarray]]:
        if not self._cache_pr:
            pr = self._precision_recall(self.pps, self.gts)
            self._pr = pr
            self._cache_pr = True

        return self._pr

    @property
    def roc(self) -> Dict[str, Union[Dict[str, Dict[str, ndarray]], Dict[str, Dict[Tuple[int, int], Dict[Any, Any]]]]]:
        if not self._cache_roc:
            roc = dict()
            roc['ovr'] = self._roc_ovr(self.pps, self.gts)
            roc['ovo'] = self._roc_ovo(self.pps, self.gts)
            self._roc = roc
            self._cache_roc = True

        return self._roc


class ClassificationMetricsWithSNRForSingle(ClassificationMetricsForSingle):
    def __init__(self, pps: np.ndarray, gts: np.ndarray, snrs: np.ndarray, classes):
        """
        Args:
            pps (np.ndarray): all items' prediction probabilities with the shape of [N, C], and sum(pps[i, :]) = 1
            gts (np.ndarray): all items' true labels with the shape of [N,], where the value should be between [0, C)
            snrs (np.ndarray): all items' snr with the shape of [N,]
            classes: list with categories' name, and the index of a specific category in the list is corresponding
            to the prediction index.
        """
        super(ClassificationMetricsWithSNRForSingle, self).__init__(pps, gts, classes)
        self.snrs = snrs
        self.snr_set = np.sort(np.unique(snrs))
        snr_to_index = {snr: index for index, snr in enumerate(self.snr_set)}
        pps_along_snr = {snr: self.pps[snrs == snr, :] for snr in snr_to_index}
        tps_along_snr = {snr: self.tps[snrs == snr, :] for snr in snr_to_index}
        gts_along_snr = {snr: self.gts[snrs == snr] for snr in snr_to_index}
        self.pps_along_snr = pps_along_snr
        self.tps_along_snr = tps_along_snr
        self.gts_along_snr = gts_along_snr

        self.num_snr = len(snr_to_index)
        self.snr_to_index = snr_to_index

    @property
    def confusion_matrix(self) -> Dict[str, np.ndarray]:
        if not self._cache_cm:
            confusion_matrix = dict()
            for snr in self.snr_to_index:
                confusion_matrix[f'{snr:02d}dB'] = self._confusion_matrix(self.pps_along_snr[snr],
                                                                          self.tps_along_snr[snr])
            # Add all snr together
            confusion_matrix['All SNR'] = np.sum(confusion_matrix[:-1, :, :], axis=0)

            self._cm = confusion_matrix
            self._cache_cm = True

        return self._cm

    @property
    def accuracy(self) -> Dict[Union[str, Any], float]:
        if not self._cache_as:
            accuracy = dict()
            confusion_matrix = self.confusion_matrix
            for snr in self.snr_to_index:
                accuracy[f'{snr:02d}dB'] = self._accuracy(confusion_matrix[self.snr_to_index[snr], :, :])
            macro_accuracy = 1.0 * sum(accuracy.values()) / len(accuracy)
            accuracy['ACC'] = macro_accuracy
            self._as = accuracy
            self._cache_as = True

        return self._as

    @property
    def f1_score(self) -> Dict[str, Dict[str, float]]:
        if not self._cache_fs:
            f1_score = dict()
            confusion_matrix = self.confusion_matrix
            for snr in self.snr_to_index:
                f1_score[f'{snr:02d}dB'] = self._f1_score(confusion_matrix[self.snr_to_index[snr], :, :])
            f1_score['All SNR'] = self._f1_score(confusion_matrix[-1, :, :])

            self._cache_fs = True

        return self._fs

    @property
    def precision_recall(self) -> Dict[Union[str, Any], Dict[str, Dict[str, ndarray]]]:
        if not self._cache_pr:
            # The function precision_recall is based on the link:
            # https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html

            pr = dict()

            for snr in self.snr_to_index:
                # For each class in different snr
                pr[snr] = self._precision_recall(self.pps_along_snr[snr], self.tps_along_snr[snr])

            pr['micro'] = self._precision_recall(self.pps, self.tps)
            self._pr = pr
            self._cache_pr = True

        return self._pr

    @property
    def roc(self) -> Dict[str, Dict[str, Dict[str, np.ndarray]]]:
        if not self._cache_roc:
            roc = dict(ovr=dict(), ovo=dict())

            # ROC using the OvR
            for snr in self.snr_to_index:
                # For each class in different snr
                roc['ovr'][snr] = self._roc_ovr(self.pps_along_snr[snr], self.tps_along_snr[snr])
                roc['ovo'][snr] = self._roc_ovo(self.pps_along_snr[snr], self.gts_along_snr[snr])

            roc['ovr']['micro'] = self._roc_ovr(self.pps, self.tps)
            roc['ovo']['micro'] = self._roc_ovo(self.pps, self.gts)
            self._roc = roc
            self._cache_roc = True

        return self._roc
